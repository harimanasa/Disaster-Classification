{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas\n",
    "from tensorflow.contrib import rnn\n",
    "import re\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(8, suppress=True)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Import and Preprocessing\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "train_corpus = pandas.read_csv(train_file, encoding='latin1').to_numpy()\n",
    "test_corpus = pandas.read_csv(test_file, encoding='latin1').to_numpy()\n",
    "train_label = train_corpus[:,4]\n",
    "train_text = train_corpus[:,3]\n",
    "#test_label = test_corpus[:,4]\n",
    "test_text = test_corpus[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hyperlinks from strings\n",
    "for idx, tweet in enumerate(train_text):\n",
    "    train_text[idx] = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "for idx, tweet in enumerate(test_text):\n",
    "    test_text[idx] = re.sub(r\"http\\S+\", \"\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Tokenization\n",
    "#Use Keras library to create a vector of words for every tweet.\n",
    "#These vectors are padded up to 50, which can be the limit of number of words possible in a 140 character tweet.\n",
    "\n",
    "    # Prepare tokenizer (t for training set, tt for test set)\n",
    "docs = train_text\n",
    "test_docs = test_text\n",
    "t = Tokenizer()\n",
    "tt = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "tt.fit_on_texts(test_docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "test_vocab_size = len(tt.word_index) + 1\n",
    "# Integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "encoded_test_docs = tt.texts_to_sequences(test_docs)\n",
    "# Pad documents to a max length of 50 words (140 characters, 3 characters a word (including space))\n",
    "max_length = 50\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding\n",
    "#For the embedding matrix we use GloVe’s 50d word vector pre-trained on 2 billion tweets. The\n",
    "#embedding matrix tabulates how frequently word’s co-occur with one another in a given corpus\n",
    " \n",
    "    # load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.twitter.27B.50d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# create a weight matrix for words in test docs\n",
    "test_embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, j in tt.word_index.items():\n",
    "    test_embedding_vector = embeddings_index.get(word)\n",
    "    if test_embedding_vector is not None:\n",
    "        test_embedding_matrix[j] = test_embedding_vector\n",
    "            \n",
    "embedding_matrix_transpose = embedding_matrix.transpose()\n",
    "test_embedding_matrix_transpose = test_embedding_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
