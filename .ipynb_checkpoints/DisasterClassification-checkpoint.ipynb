{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import re\n",
    "import csv\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(8, suppress=True)\n",
    "#These options determine the way floating point numbers, arrays and other NumPy objects are displayed.\n",
    "# 8 in first param indicates - Number of digits of precision for floating point output (default 8).\n",
    "# suppressbool, optional:-\n",
    "# If True, always print floating point numbers using fixed point notation, in which case numbers equal to zero in the current precision will print as zero. If False, then scientific notation is used when absolute value of the smallest number is < 1e-4 or the ratio of the maximum absolute value to the minimum is > 1e3. The default is False.\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib inline sets the backend of matplotlib to the 'inline' backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    }
   ],
   "source": [
    "#Loading train data\n",
    "train_file = 'data/train.csv'\n",
    "train_corpus = pandas.read_csv(train_file, encoding='latin1').to_numpy()\n",
    "\n",
    "print(train_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, nan, nan,\n",
       "        'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       "        1],\n",
       "       [4, nan, nan, 'Forest fire near La Ronge Sask. Canada', 1],\n",
       "       [5, nan, nan,\n",
       "        \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
       "        1],\n",
       "       [6, nan, nan,\n",
       "        '13,000 people receive #wildfires evacuation orders in California ',\n",
       "        1],\n",
       "       [7, nan, nan,\n",
       "        'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ',\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Head of train data to verify\n",
    "train_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10869, nan, nan,\n",
       "        'Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5',\n",
       "        1],\n",
       "       [10870, nan, nan,\n",
       "        '@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.',\n",
       "        1],\n",
       "       [10871, nan, nan,\n",
       "        'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ',\n",
       "        1],\n",
       "       [10872, nan, nan,\n",
       "        'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.',\n",
       "        1],\n",
       "       [10873, nan, nan,\n",
       "        'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d',\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tail of the train data to verify\n",
    "train_corpus[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_corpus[:,4] #target value (y=0 /1)\n",
    "train_text = train_corpus[:,3] #id,keyword,location,text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Removing hyperlinks from Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we observed in the head and tail of the train data, we see there are many hyperlinks in between the data.\n",
    "#So to simplify our process, we want to remove the hyperlinks from the data\n",
    "for idx, tweet in enumerate(train_text):\n",
    "    train_text[idx] = re.sub(r\"http\\S+\", \"\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10869, nan, nan,\n",
       "        'Two giant cranes holding a bridge collapse into nearby homes ',\n",
       "        1],\n",
       "       [10870, nan, nan,\n",
       "        '@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.',\n",
       "        1],\n",
       "       [10871, nan, nan, 'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. ',\n",
       "        1],\n",
       "       [10872, nan, nan,\n",
       "        'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.',\n",
       "        1],\n",
       "       [10873, nan, nan,\n",
       "        'The Latest: More Homes Razed by Northern California Wildfire - ABC News ',\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying that the hyperlinks are removed.\n",
    "train_corpus[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (7613, 5)\n",
      "strides:  (8, 60904)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  False\n",
      "fortran:  True\n",
      "data pointer: 0x1a4a3ab000\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: object\n"
     ]
    }
   ],
   "source": [
    "np.info(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "#Loading test data\n",
    "test_file = 'data/test.csv'\n",
    "test_corpus = pandas.read_csv(test_file, encoding='latin1').to_numpy()\n",
    "\n",
    "print(test_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, nan, nan, 'Just happened a terrible car crash'],\n",
       "       [2, nan, nan,\n",
       "        'Heard about #earthquake is different cities, stay safe everyone.'],\n",
       "       [3, nan, nan,\n",
       "        'there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all'],\n",
       "       [9, nan, nan, 'Apocalypse lighting. #Spokane #wildfires'],\n",
       "       [11, nan, nan, 'Typhoon Soudelor kills 28 in China and Taiwan']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Head of test data to verify\n",
    "test_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10861, nan, nan,\n",
       "        'EARTHQUAKE SAFETY LOS ANGELES Â\\x89Ã\\x9bÃ\\x92 SAFETY FASTENERS XrWn'],\n",
       "       [10865, nan, nan,\n",
       "        'Storm in RI worse than last hurricane. My city&amp;3others hardest hit. My yard looks like it was bombed. Around 20000K still without power'],\n",
       "       [10868, nan, nan,\n",
       "        'Green Line derailment in Chicago http://t.co/UtbXLcBIuY'],\n",
       "       [10874, nan, nan,\n",
       "        'MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3'],\n",
       "       [10875, nan, nan,\n",
       "        '#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tail of the test data to verify\n",
    "test_corpus[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = np.zeros((3263, 1), dtype=int) #Since this is what is missing and we need to predict, we are initiating it with zeroes\n",
    "test_text = test_corpus[:,3] #id,keyword,location,text (all the given four attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Removing hyperlinks from Test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hyperlinks from strings\n",
    "\n",
    "for idx, tweet in enumerate(test_text):\n",
    "    test_text[idx] = re.sub(r\"http\\S+\", \"\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (3263, 4)\n",
      "strides:  (8, 26104)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  False\n",
      "fortran:  True\n",
      "data pointer: 0x13f311000\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: object\n"
     ]
    }
   ],
   "source": [
    "np.info(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. One-Hot encoding of label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# One-hot encode\n",
    "train_targets = train_label\n",
    "encoded_train_targets = to_categorical(train_targets)\n",
    "print(encoded_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoded_train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining inputs\n",
    "\n",
    "training_docs = train_text #training \n",
    "test_docs = test_text #test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Building a reverse encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def inverseEncoding(encoded):\n",
    "    ans = np.zeros(encoded.shape[0])\n",
    "    for idx, vector in enumerate(encoded):\n",
    "        ans[idx] = np.argmax(vector)\n",
    "    return ans\n",
    "\n",
    "print(inverseEncoding(encoded_train_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7: Tokenizer - Text pre-processing with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Tokenization\n",
    "#Use Keras library to create a vector of words for every tweet.\n",
    "#These vectors are padded up to 50, which can be the limit of number of words possible in a 150 character tweet.\n",
    "\n",
    "# Prepare tokenizer (t for training set, tt for test set)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#Constructing Tokenizer class\n",
    "t = Tokenizer() #Tokenizer class constructor for training documents\n",
    "tt = Tokenizer() #Tokenizer class constructor for testing documents\n",
    "\n",
    "\n",
    "#Fitting Tokenizer to the documents\n",
    "t.fit_on_texts(training_docs) \n",
    "tt.fit_on_texts(test_docs)\n",
    "\n",
    "\n",
    "# Integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(training_docs)\n",
    "encoded_test_docs = tt.texts_to_sequences(test_docs)\n",
    "\n",
    "\n",
    "# Pad documents to a max length of 50 words (150 characters, 3 characters a word (including space))\n",
    "max_length = 50\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_docs\n",
    "# encoded_test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set vocab size is 18099\n",
      "\n",
      "The test set vocab size is 10784\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "test_vocab_size = len(tt.word_index) + 1\n",
    "print(\"The training set vocab size is \"+ str(vocab_size))\n",
    "print(\"\\nThe test set vocab size is \"+ str(test_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn about the training data through tokenizer\n",
    "# print(t.word_counts)\n",
    "# print(t.document_count)\n",
    "# print(t.word_index)\n",
    "# print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn about the testing data through tokenizer\n",
    "# print(tt.word_counts)\n",
    "# print(tt.document_count)\n",
    "# print(tt.word_index)\n",
    "# print(tt.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Develop a model for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding\n",
    "#For the embedding matrix we use GloVe’s 50d word vector pre-trained on 2 billion tweets. The\n",
    "#embedding matrix tabulates how frequently word’s co-occur with one another in a given corpus\n",
    "\n",
    "# load the whole embedding into memory\n",
    "\n",
    "#The GLoVe link: https://nlp.stanford.edu/projects/glove/\n",
    "embeddings_index = dict()\n",
    "f = open('glove/glove.twitter.27B.50d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# create a weight matrix for words in test docs\n",
    "test_embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, j in tt.word_index.items():\n",
    "    test_embedding_vector = embeddings_index.get(word)\n",
    "    if test_embedding_vector is not None:\n",
    "        test_embedding_matrix[j] = test_embedding_vector\n",
    "            \n",
    "embedding_matrix_transpose = embedding_matrix.transpose()\n",
    "test_embedding_matrix_transpose = test_embedding_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embedding_matrix)\n",
    "embedding_matrix_transpose = embedding_matrix.transpose()\n",
    "test_embedding_matrix_transpose = test_embedding_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "#unrolled through 28 time steps in mnist , 50(=max_length) words per tweet ?\n",
    "time_steps = max_length\n",
    "#hidden LSTM units = batch size , we can also take 128 for tweets and mnist?\n",
    "# Total of 4743 word vectors in training set. To ensure constant batch size: 4743 = 3*3*17*31 = 153\n",
    "num_units= 128\n",
    "#rows of 28 pixels for mnist , 50 dimension for glove words.\n",
    "n_input= 50\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "n_classes= 2\n",
    "#size of batch = we can take 128 same as number of units.\n",
    "batch_size= num_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(x ,time_steps,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-c3e3d217c690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#defining the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforget_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_units' is not defined"
     ]
    }
   ],
   "source": [
    "#defining the network\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "\n",
    "## softmax for probability\n",
    "prob = tf.nn.softmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-ab5ee18073ae>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining learning_rate for the Adam optimizer\n",
    "learning_rate=0.001\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "#prediction = output of last LSTM time step x weights + bias\n",
    "#probability = softmax(prediction)\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "def change_shape(data,embedding_matrix_transpose):\n",
    "    '''\n",
    "    Change shape to batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "    '''\n",
    "    data1 = zeros((batch_size, time_steps, n_input))\n",
    "    for x in range(0, num_units):\n",
    "        for y in range(0, time_steps): \n",
    "            #print (data[x,y], embedding_matrix(data[x,y]))\n",
    "            #print (embedding_matrix[data[x,y]])\n",
    "            for z in range(0,n_input):\n",
    "                data1[x][y][z]= embedding_matrix_transpose[z,data[x,y]]\n",
    "    #print(data1)\n",
    "    return(data1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-0da9120156d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# use embedding matrix and one hot for batch_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mchange_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix_transpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_batch' is not defined"
     ]
    }
   ],
   "source": [
    "probabilities = np.zeros((test_label.size, 2))\n",
    "losses = np.zeros(199)\n",
    "accuracies = np.zeros(199)\n",
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    while iter<2000:\n",
    "        #batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n",
    "        # use embedding matrix and one hot for batch_x\n",
    "        batch_x, batch_y = next_batch(batch_size, padded_docs, labels)\n",
    "        batch_x= change_shape(batch_x, embedding_matrix_transpose)\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if iter %10==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "    print(\"Optimization finished!\")\n",
    "    \n",
    "    #calculating test accuracy\n",
    "    # all test labels are 0\n",
    "    test_data = padded_test_docs\n",
    "    \n",
    "    '''\n",
    "    Change shape to test_data=test_data.reshape((3263,time_steps,n_input))\n",
    "    '''\n",
    "    test_data1 = zeros((3263, time_steps, n_input))\n",
    "    for a in range(0, 1701):\n",
    "        for b in range(0, time_steps): \n",
    "            for c in range(0,n_input):\n",
    "                test_data1[a][b][c]= test_embedding_matrix_transpose[c,test_data[a,b]]\n",
    "    \n",
    "    probabilities = sess.run(prob, feed_dict={x: test_data1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Probabilities\n",
      "0\t 1\n",
      "[[0.1505156  0.8494844 ]\n",
      " [0.09050537 0.9094946 ]\n",
      " [0.01507206 0.984928  ]\n",
      " ...\n",
      " [0.82812697 0.1718731 ]\n",
      " [0.82812697 0.1718731 ]\n",
      " [0.82812685 0.1718731 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    print(\"Testing Probabilities\")\n",
    "    print(\"0\\t 1\")\n",
    "    print(probabilities)\n",
    "    probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(   0, 0.1505156 , 0.8494844) (   1, 0.09050537, 0.9094946)\n",
      " (   2, 0.01507206, 0.984928 ) ... (3260, 0.82812697, 0.1718731)\n",
      " (3261, 0.82812697, 0.1718731) (3262, 0.82812685, 0.1718731)]\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(test_label.size, dtype=np.int16)\n",
    "idx.shape\n",
    "out = np.rec.fromarrays((idx, probabilities[:,0], probabilities[:,1]),  names = ('i','D','ND'))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1) (2, 1) (3, 1) ... (10868, 0) (10874, 0) (10875, 0)]\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(test_label.size, dtype=np.int16)\n",
    "idx.shape\n",
    "results = np.rec.fromarrays((test_corpus[:,0], np.where(probabilities[:,1]>0.5, 1, 0)),  names = ('id','target'))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_NONE)\n",
    "    wr.writerow(('id','target'))\n",
    "    wr.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, ..., 10868, 10874, 10875], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-5f7b54c43cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Plot Loss and Accuracy vs iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train Log Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train Accuracies'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "iterations = np.arange(start=10, stop=2000, step=10, dtype=np.int16)\n",
    "# Plot Loss and Accuracy vs iterations\n",
    "plt.plot( iterations, losses )\n",
    "plt.plot( iterations, accuracies )\n",
    "plt.legend(['Train Log Loss', 'Train Accuracies'])\n",
    "plt.ylabel('Log Loss, Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Training Loss, Accuracy vs Iterations at 128 Batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
