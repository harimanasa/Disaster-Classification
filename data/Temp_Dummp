3[1]
train_vocab_size = len(t.word_index) + 1
test_vocab_size = len(tt.word_index) + 1




Resources:
1) Tokenizer

https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/

https://keras.io/preprocessing/text/

https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer

https://towardsdatascience.com/text-classification-in-keras-part-2-how-to-use-the-keras-tokenizer-word-representations-fd571674df23


We used GloVe’s 50d word vector, which is pre-trained on 2 billion tweets, for the embedding matrix. This matrix reports the frequency of each word co-occurring with another one from the given set. We chose the widely popular GloVe’s pre-trained matrix for this work. We also chose a 50-dimension matrix to be a balance between speed of the process and embedding quality.